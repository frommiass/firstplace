"""
SearchEngine - –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –í–ï–°–ê!

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
1. 3 —ç–º–±–µ–¥–¥–µ—Ä–∞ (fast/quality/best)
2. FAISS –∏–Ω–¥–µ–∫—Å—ã
3. BM25 –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
4. Reranker –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏

–í–°–Ø –ú–ê–ì–ò–Ø –ß–ï–†–ï–ó –í–ï–°–ê!
"""

import os
import pickle
import numpy as np
from typing import List, Dict, Optional
from pathlib import Path

# –£—Å–ª–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    from rank_bm25 import BM25Okapi
    import torch
except ImportError:
    faiss = None
    SentenceTransformer = None
    BM25Okapi = None
    torch = None

from .interfaces import Chunk, SearchResult


class EmbeddingCache:
    """–ö—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self, max_size: int = 10000):
        self.cache: Dict[str, np.ndarray] = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑ –∫—ç—à–∞"""
        if text in self.cache:
            self.hits += 1
            return self.cache[text]
        else:
            self.misses += 1
            return None
    
    def put(self, text: str, embedding: np.ndarray) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ –∫—ç—à"""
        if len(self.cache) >= self.max_size:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[text] = embedding
    
    def get_hit_rate(self) -> float:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ hit rate"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0


class SearchEngine:
    """
    –ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —á–µ—Ä–µ–∑ –í–ï–°–ê!
    
    –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
    1. 3 —ç–º–±–µ–¥–¥–µ—Ä–∞ (fast/quality/best)
    2. FAISS –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–º–±–µ–¥–¥–µ—Ä–∞
    3. BM25 –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    4. Reranker –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    """
    
    def __init__(self, weights_dir: str, cache_dir: str = "./cache"):
        self.weights_dir = Path(weights_dir)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        if faiss is None or SentenceTransformer is None:
            raise RuntimeError("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: faiss-cpu, sentence-transformers, rank-bm25")
        
        # –ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_cache = EmbeddingCache()
        
        # –≠–º–±–µ–¥–¥–µ—Ä—ã
        self.embeddings = {}
        self.faiss_indices = {}
        self.bm25_indices = {}
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞
        self.dialogue_chunks: Dict[str, List[Chunk]] = {}
        self.dialogue_texts: Dict[str, List[str]] = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        self._load_models()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("    –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–æ–≤...")
        
        # Fast —ç–º–±–µ–¥–¥–µ—Ä (–±—ã—Å—Ç—Ä—ã–π)
        try:
            self.embeddings['fast'] = SentenceTransformer(
                str(self.weights_dir / "multilingual-e5-small"),
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            print("    ‚úì Fast —ç–º–±–µ–¥–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ fast —ç–º–±–µ–¥–¥–µ—Ä–∞: {e}")
        
        # Quality —ç–º–±–µ–¥–¥–µ—Ä (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π)
        try:
            self.embeddings['quality'] = SentenceTransformer(
                str(self.weights_dir / "paraphrase-multilingual-mpnet"),
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            print("    ‚úì Quality —ç–º–±–µ–¥–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ quality —ç–º–±–µ–¥–¥–µ—Ä–∞: {e}")
        
        # Best —ç–º–±–µ–¥–¥–µ—Ä (–ª—É—á—à–∏–π)
        try:
            self.embeddings['best'] = SentenceTransformer(
                str(self.weights_dir / "rubert-tiny2"),
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
            print("    ‚úì Best —ç–º–±–µ–¥–¥–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ best —ç–º–±–µ–¥–¥–µ—Ä–∞: {e}")
        
        # Reranker
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(
                str(self.weights_dir / "bge-reranker-base")
            )
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(
                str(self.weights_dir / "bge-reranker-base")
            )
            if torch.cuda.is_available():
                self.reranker_model = self.reranker_model.cuda()
            print("    ‚úì Reranker –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ reranker: {e}")
            self.reranker_model = None
    
    def build_index(self, chunks: List[Chunk], dialogue_id: str):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞"""
        if not chunks:
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏
        self.dialogue_chunks[dialogue_id] = chunks
        texts = [chunk.content for chunk in chunks]
        self.dialogue_texts[dialogue_id] = texts
        
        # –°–æ–∑–¥–∞–µ–º BM25 –∏–Ω–¥–µ–∫—Å
        tokenized_texts = [text.split() for text in texts]
        self.bm25_indices[dialogue_id] = BM25Okapi(tokenized_texts)
        
        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–º–±–µ–¥–¥–µ—Ä–∞
        for emb_name, embedder in self.embeddings.items():
            if embedder is None:
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = self._get_embeddings(texts, embedder)
            
            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            faiss.normalize_L2(embeddings)
            index.add(embeddings)
            
            self.faiss_indices[f"{dialogue_id}_{emb_name}"] = index
    
    def _get_embeddings(self, texts: List[str], embedder) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        embeddings = []
        
        for text in texts:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached = self.embedding_cache.get(text)
            if cached is not None:
                embeddings.append(cached)
            else:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = embedder.encode([text], convert_to_numpy=True)[0]
                self.embedding_cache.put(text, embedding)
                embeddings.append(embedding)
        
        return np.array(embeddings)
    
    def search(self, question: str, dialogue_id: str, top_k: int = 20) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –í–ï–°–ê!
        
        1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (3 —ç–º–±–µ–¥–¥–µ—Ä–∞)
        2. BM25 –ø–æ–∏—Å–∫
        3. Reranker –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        """
        if dialogue_id not in self.dialogue_chunks:
            return []
        
        chunks = self.dialogue_chunks[dialogue_id]
        texts = self.dialogue_texts[dialogue_id]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_results = []
        
        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–π —ç–º–±–µ–¥–¥–µ—Ä
        for emb_name, embedder in self.embeddings.items():
            if embedder is None:
                continue
            
            index_key = f"{dialogue_id}_{emb_name}"
            if index_key not in self.faiss_indices:
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–∞
            question_emb = embedder.encode([question], convert_to_numpy=True)[0]
            question_emb = question_emb.reshape(1, -1)
            faiss.normalize_L2(question_emb)
            
            # –ü–æ–∏—Å–∫ –≤ FAISS
            index = self.faiss_indices[index_key]
            scores, indices = index.search(question_emb, min(top_k, len(chunks)))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for score, idx in zip(scores[0], indices[0]):
                if idx < len(chunks):
                    result = SearchResult(
                    chunk=chunks[idx],
                        score=float(score),
                        final_score=float(score)  # –ü–æ–∫–∞ –±–µ–∑ reranker
                    )
                    all_results.append(result)
        
        # 2. BM25 –ø–æ–∏—Å–∫
        if dialogue_id in self.bm25_indices:
            bm25_scores = self.bm25_indices[dialogue_id].get_scores(question.split())
            
            for i, score in enumerate(bm25_scores):
                if i < len(chunks):
                    result = SearchResult(
                        chunk=chunks[i],
                        score=float(score),
                        final_score=float(score)  # –ü–æ–∫–∞ –±–µ–∑ reranker
                    )
                    all_results.append(result)
        
        # 3. Reranker (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.reranker_model is not None and all_results:
            all_results = self._rerank_results(question, all_results)
        
        # 4. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        unique_results = self._deduplicate_results(all_results)
        unique_results.sort(key=lambda x: x.final_score, reverse=True)
        
        return unique_results[:top_k]
    
    def _rerank_results(self, question: str, results: List[SearchResult]) -> List[SearchResult]:
        """Reranker –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ - –ó–ê–ì–õ–£–®–ï–ù –î–õ–Ø –û–¢–õ–ê–î–ö–ò"""
        if not results:
            return results
        
        print(f"\nüîç RERANKER DEBUG - –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
        print(f"   –í–æ–ø—Ä–æ—Å: '{question}'")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
        
        # –í—ã–≤–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä—ã [–≤–æ–ø—Ä–æ—Å, —Ñ—Ä–∞–≥–º–µ–Ω—Ç] –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–∞—é—Ç—Å—è –≤ reranker
        print(f"\nüìã –í—Å–µ –ø–∞—Ä—ã –¥–ª—è reranker:")
        pairs = []
        for i, result in enumerate(results):
            pair = [question, result.chunk.content]
            pairs.append(pair)
            print(f"   [{i+1:2d}] –í–æ–ø—Ä–æ—Å: '{question[:50]}...'")
            print(f"       –§—Ä–∞–≥–º–µ–Ω—Ç: '{result.chunk.content[:100]}...'")
            print(f"       –ò—Å—Ö–æ–¥–Ω—ã–π —Å–∫–æ—Ä: {result.score:.4f}")
            print(f"       –°–µ—Å—Å–∏—è: {result.chunk.session_id}")
            print()
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(results)}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {sum(r.score for r in results) / len(results):.4f}")
        print(f"   –ú–∏–Ω —Å–∫–æ—Ä: {min(r.score for r in results):.4f}")
        print(f"   –ú–∞–∫—Å —Å–∫–æ—Ä: {max(r.score for r in results):.4f}")
        
        # –ó–ê–ì–õ–£–®–ö–ê: –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–∫–æ—Ä—ã
        print(f"\n‚ö†Ô∏è  RERANKER –ó–ê–ì–õ–£–®–ï–ù - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–∫–æ—Ä—ã")
        for result in results:
            result.final_score = result.score
        
        return results
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        seen_content = set()
        unique_results = []
        
        for result in results:
            content = result.chunk.content
            if content not in seen_content:
                seen_content.add(content)
                unique_results.append(result)
        
        return unique_results