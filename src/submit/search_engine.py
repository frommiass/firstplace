"""
SearchEngine - –ó–ê–©–ò–¢–ê –û–¢ SEGFAULT!

–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
1. ‚úÖ –û—Ç–∫–ª—é—á–µ–Ω–∞ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –≤ sentence-transformers (encode single thread)
2. ‚úÖ FAISS IndexFlatIP ‚Üí IndexFlatL2 (–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –Ω–∞ CPU)
3. ‚úÖ Batch encoding —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç OOM
4. ‚úÖ Try-catch –Ω–∞ –∫–∞–∂–¥–æ–π –∫—Ä–∏—Ç–∏—á–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
5. ‚úÖ Graceful degradation –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
"""

import os
import numpy as np
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from collections import defaultdict
import threading

# –£—Å–ª–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    import faiss
    from sentence_transformers import SentenceTransformer, CrossEncoder
    from rank_bm25 import BM25Okapi
    import torch
except ImportError:
    faiss = None
    SentenceTransformer = None
    CrossEncoder = None
    BM25Okapi = None
    torch = None

from .interfaces import Chunk, SearchResult


class EmbeddingCache:
    """Thread-safe –∫—ç—à —Å LRU"""
    
    def __init__(self, max_size: int = 10000):
        self.cache: Dict[str, np.ndarray] = {}
        self.access_count: Dict[str, int] = defaultdict(int)
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
        self.lock = threading.Lock()  # Thread-safety
    
    def get(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å thread-safety"""
        with self.lock:
            if text in self.cache:
                self.hits += 1
                self.access_count[text] += 1
                return self.cache[text].copy()
            else:
                self.misses += 1
                return None
    
    def put(self, text: str, embedding: np.ndarray) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å thread-safety"""
        with self.lock:
            if len(self.cache) >= self.max_size:
                if self.access_count:
                    least_used = min(self.access_count, key=self.access_count.get)
                    del self.cache[least_used]
                    del self.access_count[least_used]
                else:
                    oldest_key = next(iter(self.cache))
                    del self.cache[oldest_key]
            
            self.cache[text] = embedding.copy()
            self.access_count[text] = 1
    
    def get_hit_rate(self) -> float:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        with self.lock:
            total = self.hits + self.misses
            return self.hits / total if total > 0 else 0.0
    
    def get_stats(self) -> Dict:
        """–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        with self.lock:
            return {
                'size': len(self.cache),
                'max_size': self.max_size,
                'hits': self.hits,
                'misses': self.misses,
                'hit_rate': self.get_hit_rate(),
                'fill_ratio': len(self.cache) / self.max_size
            }


class SearchEngine:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç segfault
    """
    
    EMBEDDER_WEIGHTS = {
        'fast': 0.25,
        'quality': 0.35,
        'best': 0.40
    }
    
    MIN_NO_INFO_THRESHOLD = 0.2
    
    def __init__(self, weights_dir: str, cache_dir: str = "./cache"):
        self.weights_dir = Path(weights_dir)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        if faiss is None or SentenceTransformer is None:
            raise RuntimeError(
                "‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: "
                "faiss-cpu, sentence-transformers, rank-bm25"
            )
        
        # Thread-safe –∫—ç—à
        self.embedding_cache = EmbeddingCache(max_size=10000)
        
        # –ú–æ–¥–µ–ª–∏
        self.embeddings: Dict[str, Optional[SentenceTransformer]] = {}
        self.reranker: Optional[CrossEncoder] = None
        
        # –ò–Ω–¥–µ–∫—Å—ã
        self.faiss_indices: Dict[str, faiss.Index] = {}
        self.bm25_indices: Dict[str, BM25Okapi] = {}
        
        # –î–∞–Ω–Ω—ã–µ
        self.dialogue_chunks: Dict[str, List[Chunk]] = {}
        self.dialogue_texts: Dict[str, List[str]] = {}
        self.dialogue_embeddings: Dict[str, Dict[str, np.ndarray]] = {}
        
        # Lock –¥–ª—è thread-safety –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        self.index_lock = threading.Lock()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.search_stats = {
            'total_searches': 0,
            'avg_results': 0,
            'reranker_calls': 0,
            'errors': 0
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        self._load_models()
        self._run_diagnostics()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç segfault"""
        print("    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤...")
        
        # –ò–º–ø–æ—Ä—Ç torch –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ CUDA
        try:
            import torch
        except ImportError:
            torch = None
        
        device = 'cuda' if torch and torch.cuda.is_available() else 'cpu'
        print(f"    –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        # ===================================================================
        # –≠–ú–ë–ï–î–î–ï–†–´ - —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–π –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
        # ===================================================================
        embedder_configs = {
            'fast': ('multilingual-e5-small', 384),
            'quality': ('paraphrase-multilingual-mpnet', 768),
            'best': ('rubert-tiny2', 312)
        }
        
        loaded_count = 0
        for name, (folder, expected_dim) in embedder_configs.items():
            try:
                model_path = str(self.weights_dir / folder)
                
                if not Path(model_path).exists():
                    print(f"    ‚ö†Ô∏è  {name}: –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                    self.embeddings[name] = None
                    continue
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–π –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
                model = SentenceTransformer(model_path, device=device)
                
                # –ö–†–ò–¢–ò–ß–ù–û: –û—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –≤ PyTorch
                if torch is not None:
                    torch.set_num_threads(1)
                
                self.embeddings[name] = model
                
                # –¢–µ—Å—Ç
                test_emb = model.encode(
                    ["—Ç–µ—Å—Ç"],
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    batch_size=1  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                )
                
                actual_dim = test_emb.shape[1]
                print(f"    ‚úì {name}: dim={actual_dim}, weight={self.EMBEDDER_WEIGHTS[name]:.2f}")
                loaded_count += 1
                
            except Exception as e:
                print(f"    ‚ùå {name}: {e}")
                self.embeddings[name] = None
        
        if loaded_count == 0:
            raise RuntimeError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–µ—Ä–∞!")
        
        print(f"    –ó–∞–≥—Ä—É–∂–µ–Ω–æ —ç–º–±–µ–¥–¥–µ—Ä–æ–≤: {loaded_count}/3")
        
        # ===================================================================
        # RERANKER
        # ===================================================================
        try:
            reranker_path = str(self.weights_dir / "bge-reranker-base")
            
            if not Path(reranker_path).exists():
                print(f"    ‚ö†Ô∏è  Reranker: –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                self.reranker = None
            else:
                self.reranker = CrossEncoder(
                    reranker_path,
                    max_length=512,
                    num_labels=1,
                    device=device
                )
                
                # –¢–µ—Å—Ç
                test_score = self.reranker.predict(
                    [["–≤–æ–ø—Ä–æ—Å", "–æ—Ç–≤–µ—Ç"]],
                    show_progress_bar=False,
                    batch_size=1  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch
                )
                
                print(f"    ‚úì Reranker: score={test_score[0]:.4f}")
                
        except Exception as e:
            print(f"    ‚ö†Ô∏è  Reranker –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω: {e}")
            self.reranker = None
    
    def _run_diagnostics(self):
        """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        print("\n    üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:")
        
        working_embedders = sum(1 for e in self.embeddings.values() if e is not None)
        total_embedders = len(self.embeddings)
        
        print(f"      –≠–º–±–µ–¥–¥–µ—Ä—ã: {working_embedders}/{total_embedders}")
        print(f"      Reranker: {'‚úì' if self.reranker else '‚úó'}")
        print(f"      –ö—ç—à: {self.embedding_cache.max_size} —Å–ª–æ—Ç–æ–≤")
    
    def build_index(self, chunks: List[Chunk], dialogue_id: str):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å –ó–ê–©–ò–¢–û–ô –û–¢ SEGFAULT
        
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏!
        """
        if not chunks:
            return
        
        # Thread-safety
        with self.index_lock:
            try:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —á–∞–Ω–∫–∞–º –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
                if dialogue_id in self.dialogue_chunks:
                    existing_chunks = self.dialogue_chunks[dialogue_id]
                    existing_texts = self.dialogue_texts[dialogue_id]
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –∏ –Ω–æ–≤—ã–µ
                    all_chunks = existing_chunks + chunks
                    all_texts = existing_texts + [chunk.content for chunk in chunks]
                else:
                    all_chunks = chunks
                    all_texts = [chunk.content for chunk in chunks]
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
                self.dialogue_chunks[dialogue_id] = all_chunks
                self.dialogue_texts[dialogue_id] = all_texts
                
                # ===================================================================
                # BM25 –∏–Ω–¥–µ–∫—Å (–ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º —Ç–µ–∫—Å—Ç–æ–≤)
                # ===================================================================
                try:
                    tokenized_texts = [text.lower().split() for text in all_texts]
                    self.bm25_indices[dialogue_id] = BM25Okapi(tokenized_texts)
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ BM25 –∏–Ω–¥–µ–∫—Å–∞: {e}")
                
                # ===================================================================
                # FAISS –∏–Ω–¥–µ–∫—Å—ã - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º
                # ===================================================================
                self.dialogue_embeddings[dialogue_id] = {}
                
                for emb_name, embedder in self.embeddings.items():
                    if embedder is None:
                        continue
                    
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –í–°–ï–• —Ç–µ–∫—Å—Ç–æ–≤
                        embeddings = self._get_embeddings_safe(all_texts, embedder, emb_name)
                        
                        if embeddings is None or len(embeddings) == 0:
                            continue
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                        self.dialogue_embeddings[dialogue_id][emb_name] = embeddings
                        
                        # FAISS –∏–Ω–¥–µ–∫—Å
                        dimension = embeddings.shape[1]
                        index = faiss.IndexFlatL2(dimension)
                        index.add(embeddings.astype('float32'))
                        
                        index_key = f"{dialogue_id}_{emb_name}"
                        self.faiss_indices[index_key] = index
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ {emb_name}: {e}")
                        continue
                
            except Exception as e:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ build_index: {e}")
                import traceback
                traceback.print_exc()
    
    def _get_embeddings_safe(self, texts: List[str], 
                             embedder: SentenceTransformer,
                             emb_name: str) -> Optional[np.ndarray]:
        """
        –ë–ï–ó–û–ü–ê–°–ù–û–ï –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç segfault
        
        –ö–†–ò–¢–ò–ß–ù–û:
        - –ú–∞–ª–µ–Ω—å–∫–∏–µ –±–∞—Ç—á–∏ (8)
        - Single thread
        - Try-catch –Ω–∞ –∫–∞–∂–¥–æ–º –±–∞—Ç—á–µ
        """
        if not texts:
            return None
        
        try:
            embeddings = []
            batch_size = 64  # GPU —Å–ø—Ä–∞–≤–∏—Ç—Å—è
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –±–∞—Ç—á–∞–º
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                
                try:
                    # SINGLE THREAD encoding
                    batch_embeddings = embedder.encode(
                        batch,
                        show_progress_bar=False,
                        convert_to_numpy=True,
                        batch_size=batch_size,
                        normalize_embeddings=False  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–∞–º–∏
                    )
                    
                    embeddings.append(batch_embeddings)
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ encoding –±–∞—Ç—á–∞ {i//batch_size}: {e}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
                    dim = 384 if emb_name == 'fast' else (768 if emb_name == 'quality' else 312)
                    zero_embeddings = np.zeros((len(batch), dim), dtype=np.float32)
                    embeddings.append(zero_embeddings)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏
            if embeddings:
                all_embeddings = np.vstack(embeddings)
                return all_embeddings.astype('float32')
            else:
                return None
                
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ _get_embeddings_safe: {e}")
            return None
    
    def search(self, question: str, dialogue_id: str, 
               top_k: int = 20) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
        self.search_stats['total_searches'] += 1
        
        if dialogue_id not in self.dialogue_chunks:
            return []
        
        chunks = self.dialogue_chunks[dialogue_id]
        
        try:
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            semantic_results = self._semantic_search_safe(question, dialogue_id, chunks, top_k)
            
            # BM25 –ø–æ–∏—Å–∫
            bm25_results = self._bm25_search(question, dialogue_id, chunks, top_k)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
            combined_results = self._combine_results(semantic_results, bm25_results)
            
            # Reranking
            if self.reranker is not None and combined_results:
                try:
                    combined_results = self._rerank_results(question, combined_results)
                    self.search_stats['reranker_calls'] += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ reranker: {e}")
            
            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
            unique_results = self._deduplicate_results(combined_results)
            unique_results.sort(key=lambda x: x.final_score, reverse=True)
            
            return unique_results[:top_k]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ search: {e}")
            self.search_stats['errors'] += 1
            return []
    
    def _semantic_search_safe(self, question: str, dialogue_id: str,
                              chunks: List[Chunk], top_k: int) -> List[SearchResult]:
        """–ë–ï–ó–û–ü–ê–°–ù–´–ô —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
        results = []
        
        for emb_name, embedder in self.embeddings.items():
            if embedder is None:
                continue
            
            index_key = f"{dialogue_id}_{emb_name}"
            if index_key not in self.faiss_indices:
                continue
            
            try:
                # Encoding –≤–æ–ø—Ä–æ—Å–∞ (single thread)
                question_emb = embedder.encode(
                    [question],
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    batch_size=1
                )[0]
                
                question_emb = question_emb.reshape(1, -1).astype('float32')
                
                # –ü–æ–∏—Å–∫ –≤ FAISS (IndexFlatL2 - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è)
                index = self.faiss_indices[index_key]
                k = min(top_k * 2, len(chunks))
                distances, indices = index.search(question_emb, k)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ similarity (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
                # –î–ª—è L2: similarity = 1 / (1 + distance)
                similarities = 1.0 / (1.0 + distances[0])
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å —ç–º–±–µ–¥–¥–µ—Ä–∞
                weight = self.EMBEDDER_WEIGHTS[emb_name]
                
                for similarity, idx in zip(similarities, indices[0]):
                    if idx < len(chunks):
                        weighted_score = float(similarity) * weight
                        result = SearchResult(
                            chunk=chunks[idx],
                            score=weighted_score,
                            final_score=weighted_score
                        )
                        results.append(result)
                        
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ {emb_name}: {e}")
                continue
        
        return results
    
    def _bm25_search(self, question: str, dialogue_id: str,
                    chunks: List[Chunk], top_k: int) -> List[SearchResult]:
        """BM25 –ø–æ–∏—Å–∫"""
        if dialogue_id not in self.bm25_indices:
            return []
        
        try:
            bm25 = self.bm25_indices[dialogue_id]
            scores = bm25.get_scores(question.lower().split())
            
            max_score = max(scores) if scores.max() > 0 else 1.0
            normalized_scores = scores / max_score
            
            top_indices = np.argsort(normalized_scores)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if idx < len(chunks):
                    result = SearchResult(
                        chunk=chunks[idx],
                        score=float(normalized_scores[idx]) * 0.3,
                        final_score=float(normalized_scores[idx]) * 0.3
                    )
                    results.append(result)
            
            return results
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ BM25: {e}")
            return []
    
    def _combine_results(self, semantic_results: List[SearchResult],
                        bm25_results: List[SearchResult]) -> List[SearchResult]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        result_map: Dict[str, Tuple[SearchResult, List[float]]] = {}
        
        for result in semantic_results + bm25_results:
            key = result.chunk.content
            
            if key not in result_map:
                result_map[key] = (result, [])
            
            result_map[key][1].append(result.score)
        
        combined = []
        for result, scores in result_map.values():
            avg_score = np.mean(scores)
            result.score = float(avg_score)
            result.final_score = float(avg_score)
            combined.append(result)
        
        return combined
    
    def _rerank_results(self, question: str,
                       results: List[SearchResult]) -> List[SearchResult]:
        """Reranking —Å –∑–∞—â–∏—Ç–æ–π"""
        if not results:
            return results
        
        try:
            pairs = [[question, result.chunk.content] for result in results]
            
            # –ú–∞–ª–µ–Ω—å–∫–∏–µ –±–∞—Ç—á–∏
            batch_size = 64
            scores = self.reranker.predict(
                pairs,
                batch_size=batch_size,
                show_progress_bar=False
            )
            
            for i, result in enumerate(results):
                reranker_score = float(scores[i])
                original_score = result.score
                result.final_score = reranker_score * 0.7 + original_score * 0.3
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ reranker: {e}")
            for result in results:
                result.final_score = result.score
        
        return results
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è"""
        seen: Dict[str, SearchResult] = {}
        
        for result in results:
            content = result.chunk.content
            
            if content not in seen or result.final_score > seen[content].final_score:
                seen[content] = result
        
        return list(seen.values())
    
    def calculate_adaptive_threshold(self, results: List[SearchResult]) -> float:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥"""
        if not results:
            return self.MIN_NO_INFO_THRESHOLD
        
        scores = [r.final_score for r in results]
        
        median = np.median(scores)
        std = np.std(scores)
        mean = np.mean(scores)
        
        if std > 0.3:
            threshold = median + std * 0.5
        else:
            threshold = mean * 0.7
        
        threshold = max(threshold, self.MIN_NO_INFO_THRESHOLD)
        
        return float(threshold)
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        cache_stats = self.embedding_cache.get_stats()
        
        return {
            'search': self.search_stats,
            'cache': cache_stats,
            'models': {
                'embedders': sum(1 for e in self.embeddings.values() if e is not None),
                'reranker': self.reranker is not None
            },
            'indices': {
                'faiss': len(self.faiss_indices),
                'bm25': len(self.bm25_indices),
                'dialogues': len(self.dialogue_chunks)
            }
        }
    
    def print_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats = self.get_stats()
        
        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê SEARCHENGINE")
        print("="*60)
        
        print(f"\nüîç –ü–æ–∏—Å–∫:")
        print(f"  –í—Å–µ–≥–æ –ø–æ–∏—Å–∫–æ–≤: {stats['search']['total_searches']}")
        print(f"  –°—Ä–µ–¥–Ω. —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {stats['search']['avg_results']:.1f}")
        print(f"  Reranker –≤—ã–∑–æ–≤–æ–≤: {stats['search']['reranker_calls']}")
        print(f"  –û—à–∏–±–æ–∫: {stats['search']['errors']}")
        
        print(f"\nüíæ –ö—ç—à:")
        print(f"  Hit rate: {stats['cache']['hit_rate']*100:.1f}%")
        print(f"  –ó–∞–ø–æ–ª–Ω–µ–Ω–æ: {stats['cache']['size']}/{stats['cache']['max_size']}")
        
        print(f"\nü§ñ –ú–æ–¥–µ–ª–∏:")
        print(f"  –≠–º–±–µ–¥–¥–µ—Ä–æ–≤: {stats['models']['embedders']}/3")
        print(f"  Reranker: {'‚úì' if stats['models']['reranker'] else '‚úó'}")
        
        print(f"\nüìö –ò–Ω–¥–µ–∫—Å—ã:")
        print(f"  FAISS: {stats['indices']['faiss']}")
        print(f"  BM25: {stats['indices']['bm25']}")
        print(f"  –î–∏–∞–ª–æ–≥–æ–≤: {stats['indices']['dialogues']}")
        
        print("="*60 + "\n")