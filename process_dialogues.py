#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ø–æ–ª–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SearchEngine

–£–õ–£–ß–®–ï–ù–ò–Ø:
1. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π SearchEngine —Å weighted ensemble
2. ‚úÖ –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç top-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å –∏—Ö scores
3. ‚úÖ –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º
4. ‚úÖ –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É SearchEngine
5. ‚úÖ –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ NO_INFO
6. ‚úÖ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
7. ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
"""

import json
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import time

# –û—Ç–∫–ª—é—á–∞–µ–º GPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
os.environ['CUDA_VISIBLE_DEVICES'] = ''
print("üîß GPU –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã")

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append('src')

from submit.interfaces import Message
from submit.model_inference import SubmitModelWithMemory


def load_dialogues_from_json(file_path: str) -> List[Dict[str, Any]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –∏–∑ JSONL —Ñ–∞–π–ª–∞.
    
    Args:
        file_path: –ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É
        
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏
    """
    try:
        dialogues = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    if isinstance(data, dict) and 'question' in data and 'sessions' in data:
                        dialogues.append({
                            'id': data['id'],
                            'question': data['question'],
                            'answer': data['ans'],
                            'question_type': data['question_type'],
                            'sessions': data['sessions'],
                            'ans_session_ids': data.get('ans_session_ids', [])
                        })
                        
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
                    continue
        
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogues)} –¥–∏–∞–ª–æ–≥–æ–≤")
        return dialogues
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return []


def convert_to_messages(dialogue_messages: List[Dict]) -> List[Message]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ –≤ –æ–±—ä–µ–∫—Ç—ã Message.
    
    Args:
        dialogue_messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ JSON
        
    Returns:
        –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Message
    """
    messages = []
    for msg in dialogue_messages:
        message = Message(
            content=msg.get('content', ''),
            role=msg.get('role', 'user'),
            session_id=str(msg.get('session_id', 'unknown'))
        )
        messages.append(message)
    
    return messages


def calculate_similarity(text1: str, text2: str) -> float:
    """
    –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ä–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏ (Jaccard similarity).
    
    Args:
        text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
        text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
    """
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    
    return intersection / union if union > 0 else 0.0


def process_dialogue(model: SubmitModelWithMemory, 
                     dialogue: Dict[str, Any], 
                     output_dir: Path) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–∏–∞–ª–æ–≥ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º.
    
    Args:
        model: –≠–∫–∑–µ–º–ø–ª—è—Ä SubmitModelWithMemory
        dialogue: –î–∞–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∞ —Å –≤–æ–ø—Ä–æ—Å–æ–º
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    dialogue_id = dialogue['id']
    question = dialogue['question']
    correct_answer = dialogue['answer']
    question_type = dialogue['question_type']
    sessions = dialogue['sessions']
    
    print(f"\n{'='*80}")
    print(f"üìù –î–∏–∞–ª–æ–≥ {dialogue_id}")
    print(f"{'='*80}")
    print(f"–í–æ–ø—Ä–æ—Å: {question}")
    print(f"–¢–∏–ø: {question_type}")
    print(f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}")
    print(f"–°–µ—Å—Å–∏–π: {len(sessions)}")
    
    stats = {
        'dialogue_id': dialogue_id,
        'question_type': question_type,
        'success': False,
        'similarity': 0.0,
        'search_time': 0.0,
        'generation_time': 0.0,
        'total_messages': 0
    }
    
    try:
        # ===================================================================
        # 1. –ó–ê–ü–ò–°–¨ –í –ü–ê–ú–Ø–¢–¨
        # ===================================================================
        print(f"\n[1/4] –ó–∞–ø–∏—Å—å —Å–µ—Å—Å–∏–π –≤ –ø–∞–º—è—Ç—å...")
        total_messages = 0
        
        for session in sessions:
            session_id = session['id']
            messages = convert_to_messages(session['messages'])
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π dialogue_id –¥–ª—è –≤—Å–µ—Ö —Å–µ—Å—Å–∏–π
            model.write_to_memory(messages, dialogue_id)
            total_messages += len(messages)
        
        stats['total_messages'] = total_messages
        print(f"‚úì –ó–∞–ø–∏—Å–∞–Ω–æ {total_messages} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ {len(sessions)} —Å–µ—Å—Å–∏–π")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π flush
        if dialogue_id in model.write_buffer and model.write_buffer[dialogue_id]:
            model._flush_buffer(dialogue_id)
            print(f"‚úì –ë—É—Ñ–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
        
        # ===================================================================
        # 2. –ü–û–ò–°–ö –†–ï–õ–ï–í–ê–ù–¢–ù–´–• –§–†–ê–ì–ú–ï–ù–¢–û–í
        # ===================================================================
        print(f"\n[2/4] –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        search_start = time.time()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π SearchEngine –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        search_results = model.search_engine.search(
            question=question,
            dialogue_id=dialogue_id,
            top_k=20
        )
        
        search_time = time.time() - search_start
        stats['search_time'] = search_time
        
        print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ {search_time:.3f}s")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
        if search_results:
            adaptive_threshold = model.search_engine.calculate_adaptive_threshold(search_results)
            print(f"üìä –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ NO_INFO: {adaptive_threshold:.3f}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print(f"\nüìã –¢–æ–ø-5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞:")
            for i, result in enumerate(search_results[:5], 1):
                content_preview = result.chunk.content[:80]
                print(f"   {i}. [score={result.final_score:.3f}] {content_preview}...")
        
        # ===================================================================
        # 3. –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–í–ï–¢–ê
        # ===================================================================
        print(f"\n[3/4] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        generation_start = time.time()
        
        try:
            generated_answer = model.answer_to_question(dialogue_id, question)
            generation_time = time.time() - generation_start
            stats['generation_time'] = generation_time
            
            print(f"‚úì –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {generation_time:.3f}s")
            print(f"ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {generated_answer}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            generated_answer = f"–û—à–∏–±–∫–∞: {e}"
            generation_time = time.time() - generation_start
            stats['generation_time'] = generation_time
        
        # ===================================================================
        # 4. –°–†–ê–í–ù–ï–ù–ò–ï –° –ü–†–ê–í–ò–õ–¨–ù–´–ú –û–¢–í–ï–¢–û–ú
        # ===================================================================
        print(f"\n[4/4] –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞...")
        
        similarity = calculate_similarity(generated_answer, correct_answer)
        stats['similarity'] = similarity
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —É—Å–ø–µ—Ö–∞
        is_correct = similarity > 0.3 or generated_answer.lower() in correct_answer.lower()
        stats['success'] = is_correct
        
        print(f"üìä –°—Ö–æ–∂–µ—Å—Ç—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º: {similarity:.2%}")
        print(f"‚úì –°—Ç–∞—Ç—É—Å: {'‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û' if is_correct else '‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û'}")
        
        # ===================================================================
        # 5. –°–û–ó–î–ê–ù–ò–ï –î–ï–¢–ê–õ–¨–ù–û–ì–û –û–¢–ß–ï–¢–ê
        # ===================================================================
        report = []
        report.append("=" * 80)
        report.append(f"–û–¢–ß–ï–¢ –ü–û –î–ò–ê–õ–û–ì–£ {dialogue_id}")
        report.append("=" * 80)
        report.append("")
        
        report.append(f"–í–æ–ø—Ä–æ—Å: {question}")
        report.append(f"–¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: {question_type}")
        report.append("")
        
        report.append("–û–¢–í–ï–¢–´:")
        report.append(f"‚úì –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π: {correct_answer}")
        report.append(f"ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {generated_answer}")
        report.append(f"üìä –°—Ö–æ–∂–µ—Å—Ç—å: {similarity:.2%}")
        report.append(f"üìà –°—Ç–∞—Ç—É—Å: {'‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û' if is_correct else '‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û'}")
        report.append("")
        
        report.append("–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        report.append(f"‚Ä¢ –°–æ–æ–±—â–µ–Ω–∏–π –≤ –ø–∞–º—è—Ç–∏: {total_messages}")
        report.append(f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_results)}")
        report.append(f"‚Ä¢ –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {search_time:.3f}s")
        report.append(f"‚Ä¢ –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generation_time:.3f}s")
        report.append(f"‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {search_time + generation_time:.3f}s")
        
        if search_results:
            report.append(f"‚Ä¢ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: {adaptive_threshold:.3f}")
            report.append(f"‚Ä¢ –õ—É—á—à–∏–π score: {search_results[0].final_score:.3f}")
        
        report.append("")
        report.append("=" * 80)
        report.append(f"–¢–û–ü-{min(20, len(search_results))} –†–ï–õ–ï–í–ê–ù–¢–ù–´–• –§–†–ê–ì–ú–ï–ù–¢–û–í")
        report.append("=" * 80)
        report.append("")
        
        for i, result in enumerate(search_results[:20], 1):
            report.append(f"[{i}] Score: {result.final_score:.4f}")
            report.append(f"    –ò—Å—Ö–æ–¥–Ω—ã–π score: {result.score:.4f}")
            report.append(f"    –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {result.chunk.content}")
            report.append(f"    –†–æ–ª—å: {result.chunk.role}")
            report.append(f"    Session: {result.chunk.session_id}")
            
            if result.chunk.metadata:
                report.append(f"    –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {result.chunk.metadata}")
            
            report.append("")
        
        report.append("=" * 80)
        report.append("–°–¢–ê–¢–ò–°–¢–ò–ö–ê SEARCHENGINE")
        report.append("=" * 80)
        report.append("")
        
        search_stats = model.search_engine.get_stats()
        report.append(f"–ü–æ–∏—Å–∫–∏: {search_stats['search']['total_searches']}")
        report.append(f"–°—Ä–µ–¥–Ω. —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {search_stats['search']['avg_results']:.1f}")
        report.append(f"Reranker –≤—ã–∑–æ–≤–æ–≤: {search_stats['search']['reranker_calls']}")
        report.append(f"Hit rate –∫—ç—à–∞: {search_stats['cache']['hit_rate']*100:.1f}%")
        report.append(f"–≠–º–±–µ–¥–¥–µ—Ä–æ–≤: {search_stats['models']['embedders']}/3")
        report.append(f"Reranker: {'‚úì' if search_stats['models']['reranker'] else '‚úó'}")
        report.append("")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        output_file = output_dir / f"dialogue_{dialogue_id}_report.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
        
        # ===================================================================
        # 6. –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò
        # ===================================================================
        print(f"\nüßπ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
        model.clear_memory(dialogue_id)
        print(f"‚úì –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
        
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        stats['success'] = False
    
    return stats


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    try:
        print("=" * 80)
        print("üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô –°–ö–†–ò–ü–¢ –û–ë–†–ê–ë–û–¢–ö–ò –î–ò–ê–õ–û–ì–û–í")
        print("=" * 80)
        print("\n–§—É–Ω–∫—Ü–∏–∏:")
        print("‚Ä¢ Weighted ensemble –ø–æ–∏—Å–∫ (3 —ç–º–±–µ–¥–¥–µ—Ä–∞ + BM25 + reranker)")
        print("‚Ä¢ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ NO_INFO")
        print("‚Ä¢ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ SearchEngine")
        print("‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏")
        print("‚Ä¢ –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É –¥–∏–∞–ª–æ–≥—É")
        print("=" * 80)
        
        # ===================================================================
        # –ü–ê–†–ê–ú–ï–¢–†–´
        # ===================================================================
    input_file = "data/format_example.jsonl"
    output_dir = Path("output_reports")
    model_path = "path/to/gigachat"  # –ü—É—Ç—å –∫ GigaChat –º–æ–¥–µ–ª–∏
    weights_dir = "src/submit/weights"
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    output_dir.mkdir(exist_ok=True)
    print(f"\nüìÅ –û—Ç—á–µ—Ç—ã: {output_dir}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
    if not os.path.exists(input_file):
        print(f"‚ùå –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # ===================================================================
    # –ó–ê–ì–†–£–ó–ö–ê –î–ò–ê–õ–û–ì–û–í
    # ===================================================================
    print(f"\nüìñ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ {input_file}...")
    dialogues = load_dialogues_from_json(input_file)
    
    if not dialogues:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∏–∞–ª–æ–≥–∏!")
        return
    
    # ===================================================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò
    # ===================================================================
    print(f"\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SubmitModelWithMemory...")
    print(f"   Model path: {model_path}")
    print(f"   Weights dir: {weights_dir}")
    
    try:
        model = SubmitModelWithMemory(
            model_path=model_path,
            weights_dir=weights_dir
        )
        print("‚úì –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–æ–¥–µ–ª–µ–π
        model.search_engine.print_stats()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("1. –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π –≤ src/submit/weights/")
        print("2. –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
        print("   pip install torch transformers sentence-transformers")
        print("   pip install faiss-cpu rank-bm25 vllm")
        return
    
    # ===================================================================
    # –û–ë–†–ê–ë–û–¢–ö–ê –î–ò–ê–õ–û–ì–û–í
    # ===================================================================
    # –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –î–õ–Ø –û–¢–õ–ê–î–ö–ò
    dialogues_to_process = dialogues[:1]  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–∏–∞–ª–æ–≥
    print(f"\nüîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(dialogues_to_process)} –¥–∏–∞–ª–æ–≥–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)...")
    
    all_stats = []
    start_time = time.time()
    
    for i, dialogue in enumerate(dialogues_to_process, 1):
        print(f"\n{'='*80}")
        print(f"–ü–†–û–ì–†–ï–°–°: {i}/{len(dialogues)} ({i/len(dialogues)*100:.1f}%)")
        print(f"{'='*80}")
        
        stats = process_dialogue(model, dialogue, output_dir)
        all_stats.append(stats)
        
        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–∂–¥—ã–µ 10 –¥–∏–∞–ª–æ–≥–æ–≤
        if i % 10 == 0:
            correct = sum(1 for s in all_stats if s['success'])
            accuracy = correct / len(all_stats) * 100
            avg_time = sum(s['search_time'] + s['generation_time'] for s in all_stats) / len(all_stats)
            
            print(f"\nüìä –ü–†–û–ú–ï–ñ–£–¢–û–ß–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(dialogues)}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1f}% ({correct}/{len(all_stats)})")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}s")
    
    total_time = time.time() - start_time
    
    # ===================================================================
    # –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    # ===================================================================
    print(f"\n{'='*80}")
    print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*80}")
    
    total = len(all_stats)
    correct = sum(1 for s in all_stats if s['success'])
    accuracy = correct / total * 100 if total > 0 else 0
    
    avg_search_time = sum(s['search_time'] for s in all_stats) / total if total > 0 else 0
    avg_gen_time = sum(s['generation_time'] for s in all_stats) / total if total > 0 else 0
    avg_similarity = sum(s['similarity'] for s in all_stats) / total if total > 0 else 0
    
    print(f"\n‚è±Ô∏è  –í–†–ï–ú–Ø:")
    print(f"   –û–±—â–µ–µ: {total_time:.1f}s ({total_time/60:.1f} –º–∏–Ω)")
    print(f"   –°—Ä–µ–¥–Ω. –Ω–∞ –¥–∏–∞–ª–æ–≥: {total_time/total:.2f}s")
    print(f"   –°—Ä–µ–¥–Ω. –ø–æ–∏—Å–∫: {avg_search_time:.3f}s")
    print(f"   –°—Ä–µ–¥–Ω. –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {avg_gen_time:.3f}s")
    
    print(f"\nüìà –ö–ê–ß–ï–°–¢–í–û:")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {correct}/{total} ({accuracy:.1f}%)")
    print(f"   –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {avg_similarity:.2%}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –≤–æ–ø—Ä–æ—Å–æ–≤
    print(f"\nüìã –ü–û –¢–ò–ü–ê–ú –í–û–ü–†–û–°–û–í:")
    types = {}
    for s in all_stats:
        q_type = s['question_type']
        if q_type not in types:
            types[q_type] = {'total': 0, 'correct': 0}
        types[q_type]['total'] += 1
        if s['success']:
            types[q_type]['correct'] += 1
    
    for q_type, counts in sorted(types.items()):
        acc = counts['correct'] / counts['total'] * 100 if counts['total'] > 0 else 0
        print(f"   {q_type}: {counts['correct']}/{counts['total']} ({acc:.1f}%)")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ SearchEngine
    print(f"\n{'='*80}")
    model.search_engine.print_stats()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
    summary_file = output_dir / "summary.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("–°–í–û–î–ù–´–ô –û–¢–ß–ï–¢\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {total}\n")
        f.write(f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {correct} ({accuracy:.1f}%)\n")
        f.write(f"–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {avg_similarity:.2%}\n")
        f.write(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}s\n")
        f.write(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –¥–∏–∞–ª–æ–≥: {total_time/total:.2f}s\n\n")
        
        f.write("–ü–û –¢–ò–ü–ê–ú –í–û–ü–†–û–°–û–í:\n")
        for q_type, counts in sorted(types.items()):
            acc = counts['correct'] / counts['total'] * 100 if counts['total'] > 0 else 0
            f.write(f"  {q_type}: {counts['correct']}/{counts['total']} ({acc:.1f}%)\n")
    
        print(f"\nüíæ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç: {summary_file}")
        
        print(f"\nüéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"üìÅ –í—Å–µ –æ—Ç—á–µ—Ç—ã –≤: {output_dir}")
        
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –í MAIN(): {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()